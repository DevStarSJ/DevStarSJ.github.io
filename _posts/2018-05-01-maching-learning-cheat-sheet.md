---
layout: post
title: "머신러닝 알고리즘 Cheat Sheet"
subtitle: "머신러닝 알고리즘 Cheat Sheet"
categories: data
tags: ml
comments: true
---
각종 머신러닝 알고리즘의 Cheat Sheet입니다! 매번 검색하기 번거로워 인터넷에 있는 자료들을 가지고 왔습니다


## Dummies 자료

| Algorithm               | Best at                                                                                                | Pros                                                                                                                                                                                                                                                                   | Cons                                                                                                                                                                                          |
|-------------------------|--------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Random Forest           | Apt at almost any machine learning problem<br>Bioinformatics                                           | Can work in parallel<br>Seldom overfits<br>Automatically handles missing values<br>No need to transform any variable<br>No need to tweak parameters<br>Can be used by almost anyone with excellent results                                                             | Difficult to interpret<br>Weaker on regression when estimating values at the extremities of the distribution of response values<br>Biased in multiclass problems toward more frequent classes |
| Gradient Boosting       | Apt at almost any machine learning problem<br>Search engines (solving the problem of learning to rank) | It can approximate most nonlinear function<br>Best in class predictor<br>Automatically handles missing values<br>No need to transform any variable                                                                                                                     | It can overfit if run for too many iterations<br>Sensitive to noisy data and outliers<br>Doesn’t work well without parameter tuning                                                           |
| Linear regression       | Baseline predictions<br>Econometric predictions<br>Modelling marketing responses                       | Simple to understand and explain<br>It seldom overfits<br>Using L1 & L2 regularization is effective in feature selection<br>Fast to train<br>Easy to train on big data thanks to its stochastic version                                                                | You have to work hard to make it fit nonlinear functions<br>Can suffer from outliers                                                                                                          |
| Support Vector Machines | Character recognition<br>Image recognition<br>Text classification                                      | Automatic nonlinear feature creation<br>Can approximate complex nonlinear functions                                                                                                                                                                                    | Difficult to interpret when applying nonlinear kernels<br>Suffers from too many examples, after 10,000 examples it starts taking too long to train                                            |
| K-nearest Neighbors     | Computer vision<br>Multilabel tagging<br>Recommender systems<br>Spell checking problems                | Fast, lazy training<br>Can naturally handle extreme multiclass problems (like tagging text)                                                                                                                                                                            | Slow and cumbersome in the predicting phase<br>Can fail to predict correctly due to the curse of dimensionality                                                                               |
| Adaboost                | Face detection                                                                                         | Automatically handles missing values<br>No need to transform any variable<br>It doesn’t overfit easily<br>Few parameters to tweak<br>It can leverage many different weak-learners                                                                                      | Sensitive to noisy data and outliers<br>Never the best in class predictions                                                                                                                   |
| Naive Bayes             | Face recognition<br>Sentiment analysis<br>Spam detection<br>Text classification                        | Easy and fast to implement, doesn’t require too much memory and can be used for online learning<br>Easy to understand<br>Takes into account prior knowledge                                                                                                            | Strong and unrealistic feature independence assumptions<br>Fails estimating rare occurrences<br>Suffers from irrelevant features                                                              |
| Neural Networks         | Image recognition<br>Language recognition and translation<br>Speech recognition<br>Vision recognition  | Can approximate any nonlinear function<br>Robust to outliers<br>Works only with a portion of the examples (the support vectors)                                                                                                                                        | Very difficult to set up<br>Difficult to tune because of too many parameters and you have also to decide the architecture of the network<br>Difficult to interpret<br>Easy to overfit         |
| Logistic regression     | Ordering results by probability<br>Modelling marketing responses                                       | Simple to understand and explain<br>It seldom overfits<br>Using L1 & L2 regularization is effective in feature selection<br>The best algorithm for predicting probabilities of an event<br>Fast to train<br>Easy to train on big data thanks to its stochastic version | You have to work hard to make it fit nonlinear functions<br>Can suffer from outliers                                                                                                          |
| SVD                     | Recommender systems                                                                                    | Can restructure data in a meaningful way                                                                                                                                                                                                                               | Difficult to understand why data has been restructured in a certain way                                                                                                                       |
| PCA                     | Removing collinearity<br>Reducing dimensions of the dataset                                            | Can reduce data dimensionality                                                                                                                                                                                                                                         | Implies strong linear assumptions (components are a weighted summations of features)                                                                                                          |
| K-means                 | Segmentation                                                                                           | Fast in finding clusters<br>Can detect outliers in multiple dimensions                                                                                                                                                                                                 | Suffers from multicollinearity<br>Clusters are spherical, can’t detect groups of other shape<br>Unstable solutions, depends on initialization                                                 |



## Microsoft Azure Machine Learning 자료
<img src="https://www.dropbox.com/s/vkxgs8x9e5163a1/algorithm-cheat-sheet.jpg?raw=1">

## Reference
- [Dummies](http://www.dummies.com/programming/big-data/data-science/machine-learning-dummies-cheat-sheet/)
- [Microsoft Azure Machine Learning](http://aka.ms/MLCheatSheet)